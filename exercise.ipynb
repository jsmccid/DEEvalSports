{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70a3f46-149c-4311-b62d-e18f419a4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from src.secrets import secrets as sc\n",
    "from gcloud import storage\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cc2cf-bdfc-4d89-89a9-347cc20c68e1",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1468c50f-ccdb-45d0-af99-79bcb1822985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name and unique sheet ID from gdrive\n",
    "sheetIDs = [[\"People\", \"0\"], [\"Matches\", \"1626706262\"], [\"Formations\", \"900063785\"], [\"PlayByPlay\", \"1273503444\"], [\"MatchEvents\", \"567816814\"], [\"TrainingSessions\", \"960271477\"], [\"TrainingBreakdown\", \"1405062779\"], [\"TrainingEvents\", \"928411476\"]]\n",
    "\n",
    "for name, number in sheetIDs:\n",
    "    # unique identifiers stored in secrets file\n",
    "    url = f\"https://docs.google.com/spreadsheets/d/{sc.SHEET_UID}/export?format=csv&gid={number}\"\n",
    "    # read direct from gdrive\n",
    "    sheet = pd.read_csv(url)\n",
    "    # output location\n",
    "    out = f\"./data/{name}.csv\"\n",
    "    # export dropping index\n",
    "    sheet.to_csv(out, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307c443-a75e-4a63-99b8-c956f07d87ea",
   "metadata": {},
   "source": [
    "# Store in google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20191290-4739-4cba-a983-b28535007f4c",
   "metadata": {},
   "source": [
    "single region - sydney\n",
    "\n",
    "standard class\n",
    "\n",
    "public access prevention on\n",
    "\n",
    "access control uninform (simplicity)\n",
    "\n",
    "no protection\n",
    "\n",
    "encryption: google managed key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1fc1b6e-d3cf-43c4-97f5-83bb7a5be900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/MatchEvents.csv\n",
      "data/TrainingBreakdown.csv\n",
      "data/PlayByPlay.csv\n",
      "data/People.csv\n",
      "data/TrainingSessions.csv\n",
      "data/TrainingEvents.csv\n",
      "data/Formations.csv\n",
      "data/Matches.csv\n"
     ]
    }
   ],
   "source": [
    "# modified code from below as template\n",
    "# https://stackoverflow.com/questions/37003862/how-to-upload-a-file-to-google-cloud-storage-on-python-3\n",
    "\n",
    "# use pathlib to set path to data\n",
    "sheetDirectory = Path(\"./data/\")\n",
    "\n",
    "# credentials from service account key, stored in secrets\n",
    "credentials_dict = {\n",
    "    'type': 'service_account',\n",
    "    'client_id': sc.BUCKET_CLIENT_ID,\n",
    "    'client_email': sc.BUCKET_CLIENT_EMAIL,\n",
    "    'private_key_id': sc.BUCKET_PRIVATE_KEY_ID,\n",
    "    'private_key': sc.BUCKET_PRIVATE_KEY,\n",
    "}\n",
    "\n",
    "# set creds\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_dict(\n",
    "    credentials_dict\n",
    ")\n",
    "\n",
    "# start client sessions\n",
    "client = storage.Client(credentials=credentials, project=sc.BUCKET_PROJECT)\n",
    "\n",
    "# locate in project bucket\n",
    "bucket = client.get_bucket(sc.BUCKET_NAME)\n",
    "\n",
    "# iterate over all files in directory with csv as file type\n",
    "for path in sheetDirectory.glob(\"*.csv\"):\n",
    "    # set file (blob) name in bucket\n",
    "    blob = bucket.blob(path.name)\n",
    "    # upload file to blob\n",
    "    blob.upload_from_filename(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c27f7-932e-4751-b05c-f396d754e14c",
   "metadata": {},
   "source": [
    "# Create function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938369e-d6d9-47a4-a2dd-7ef7b1407d60",
   "metadata": {},
   "source": [
    "name: ETLDEEval\n",
    "\n",
    "region: sydney\n",
    "\n",
    "trigger: cloudstorage -> finalise/create (run on data change)\n",
    "\n",
    "runtime: 256mb, timeout 60\n",
    "\n",
    "runtime: python 3.9\n",
    "\n",
    "ingress settings: allow internal traffic + load balancing\n",
    "\n",
    "*would likely be better to use a data transfer / dataflow*\n",
    "\n",
    "*inefficient as replacing table rather than appending new values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb20ca16-5d37-42b6-b1fd-ec13ee8d1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcs_etl_bq(event, context):\n",
    "    \"\"\"Triggered by a change to a Cloud Storage bucket.\n",
    "    Args:\n",
    "         event (dict): Event payload.\n",
    "         context (google.cloud.functions.Context): Metadata for the event.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    \n",
    "    # set sessions\n",
    "    gcsSession = storage.Client()\n",
    "    bqSession = bigquery.Client()\n",
    "    \n",
    "    # error gate for filetype\n",
    "    if event['contentType'] == \"text/csv\":\n",
    "        # check / setup bigquery\n",
    "        # set dataset ID\n",
    "        datasetName = \"deeval\"\n",
    "        datasetID = f\"{bqSession.project}.{datasetName}\"\n",
    "        ## dataset check or creation\n",
    "        try:\n",
    "            bqSession.get_dataset(datasetID)\n",
    "        except NotFound:\n",
    "            #create dataset params\n",
    "            dataset = bigquery.Dataset(datasetID)\n",
    "            dataset.location = \"australia-southeast1\"\n",
    "            # post to bigquery to create dataset\n",
    "            datasetCreate = bqSession.create_dataset(dataset, timeout=30)\n",
    "            # wait for query to complete\n",
    "            datasetCreate.result()\n",
    "        \n",
    "        # ELT to reduce need for memory -> load to ingest table -> create actual table appending timestamp\n",
    "        #issue with all strings and autodetect -> some tables have typeFixNum column\n",
    "        \n",
    "        ## table\n",
    "        ## set table id        \n",
    "        # regex to get tablename from event metadata\n",
    "        tableName = re.match(r\".*(?=\\.csv)\", event['name']).group(0)\n",
    "        tableName = re.sub(r\"\\W\", \"\", tableName) \n",
    "        tableID = f\"{datasetID}.{tableName}\"\n",
    "        ingestTableName = f\"{tableName}ingest\"\n",
    "        ingestTableID = f\"{datasetID}.{ingestTableName}\"\n",
    "        \n",
    "        # resource location\n",
    "        uri = f\"gs://{event['bucket']}/{event['name']}\"\n",
    "        \n",
    "        # need to autodetect schema so one funciton can be used for all files\n",
    "        loadConfig = bigquery.job.LoadJobConfig(autodetect = True, # auto detect schema\n",
    "                                                skip_leading_rows = 1, # skips importing header rows\n",
    "                                                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # replaces table if existing\n",
    "                                                source_format=bigquery.SourceFormat.CSV)\n",
    "        # load data to ingest table\n",
    "        ingest = bqSession.load_table_from_uri(source_uris = uri,\n",
    "                                               destination = ingestTableID,\n",
    "                                               job_config = loadConfig,\n",
    "                                               timeout = 30)\n",
    "        # wait for completion\n",
    "        ingest.result()\n",
    "        \n",
    "        # transform adding col\n",
    "        # tested sql in bigquery before moving into python code\n",
    "        # leave timestamp as UTC\n",
    "        # SQL query to create and append table\n",
    "        transform = f\"CREATE OR REPLACE TABLE {tableID} AS SELECT *, CURRENT_TIMESTAMP() AS extracted_at FROM {ingestTableID}\"\n",
    "        \n",
    "        # post query to bigquery\n",
    "        transform = bqSession.query(transform)\n",
    "        # wait for completion\n",
    "        transform.result()\n",
    "        \n",
    "        # verify completion\n",
    "        destinationTable = bqSession.get_table(tableID)  # Make an API request.\n",
    "        print(\"Loaded {} rows.\".format(destinationTable.num_rows))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
